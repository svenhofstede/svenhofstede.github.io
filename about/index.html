<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Your site description"><link rel="shortcut icon" href=https://svenhofstede.com/favicon.ico><link rel=stylesheet href=/css/style.min.css><link rel=canonical href=https://svenhofstede.com/about/><title>About</title></head><body><header id=banner><strong><a href=https://svenhofstede.com/>Sven Hofstede</a></strong><nav><ul><li><a href=/ title=posts>posts</a></li><li><a href=/about/ title=About>About</a></li></ul></nav></header><main id=content><article><header id=post-header><div></div></header><ul><li><a href=mailto:svenhofstede@gmail.com>svenhofstede@gmail.com</a></li><li><a href=https://www.svenhofstede.com>www.svenhofstede.com</a></li></ul><h2 id=summary>Summary</h2><p>Senior+ software engineer with over 12 years of experience in both data and software engineering with some DevOps automation knowledge sprinkled on top. I&rsquo;ve worked in various roles at <strong>ASML</strong>, <strong>Nike</strong>, <strong>Johnson & Johnson</strong> and <strong>Ravago</strong>.</p><p>I have experience leading software engineering teams and building products that end users like to use. I enjoy being actively involved in the functional decision making of product design including talking to end users and other stakeholders.</p><p>I&rsquo;m expert level in building resilient and performant data pipelines in <strong>Apache (Py)Spark</strong> including running it in auto-scaling environments. I&rsquo;ve worked with data warehouse tooling like <strong>Databricks</strong>, <strong>Hive</strong> and <strong>Snowflake</strong> for years. I&rsquo;m very familiar with running and using scheduling tools like for example <strong>Airflow</strong>.</p><p>I&rsquo;m comfortable in <strong>Linux</strong> environments and dealing with software containerization (including <strong>Docker</strong> and <strong>Kubernetes</strong>). I&rsquo;m very experienced in cloud products offered by both <strong>Azure</strong> and <strong>AWS</strong> including <strong>S3</strong>, <strong>EKS</strong>, <strong>EC2</strong>, <strong>DynamoDB</strong> and how to automate setup of this using <strong>Terraform</strong>.</p><p>I&rsquo;m very comfortable building (web) applications in <strong>Python</strong> and <strong>Java</strong>. I&rsquo;ve worked on (and lead) multiple production grade applications. I understand the importance of <strong>CI/CD</strong>, <strong>automated testing</strong>, backwards compatibility, solid documentation and <strong>UX</strong> of the product.</p><p>I&rsquo;ve helped in system design and development of internal data platforms used by data teams globally.</p><p>I&rsquo;m native speaker in both English and Dutch.</p><div style=break-after:page></div><h2 id=experience>Experience</h2><h4 id=asml---data-engineer>ASML - Data engineer</h4><p><em>2023 - current</em></p><p>Helping ASML by building resilient and performant data pipelines using Databricks and PySpark, running on Azure cloud.</p><h4 id=nike---lead-software-engineer>Nike - Lead Software engineer</h4><p><em>2019 - 2023</em></p><p>Lead engineer of custom data pipeline-as-a-service solution for Nike Data & Analytics teams globally.</p><ul><li>Design and implementation of (py)spark framework, built for extensibility</li><li>All data pipelines configured and scheduled via central API (Python,FastAPI)</li><li>Uses Apache Airflow for scheduling of the Spark workloads</li><li>Data is stored on combination of S3/Hive and Databricks. Final datasets in Snowflake</li><li>Everything runs on self-managed, auto-scaling EKS clusters on AWS</li></ul><h4 id=ravago---senior-software-engineer>Ravago - Senior Software engineer</h4><p><em>2017 - 2019</em></p><p>Development of new features of in-house ERP system based on Spring Boot</p><ul><li>Investigating bug reports and implementing fixes</li><li>Code reviews, technical analysis and review functional analysis</li><li>Lead administrator of multiple Kubernetes clusters</li><li>Automation of development pipelines using Bamboo and Jenkins</li></ul><h4 id=johnson--johnson---big-data-engineer>Johnson & Johnson - Big Data engineer</h4><p><em>2012 - 2017</em></p><ul><li>Develop ingestion mechanism into Data lake (CSV, Databases,SharePoint, REST, ..) in Cloudera cluster</li><li>Develop datamarts using Apache Spark (Scala and Python) and Hive/Impala.</li></ul><h2 id=education>Education</h2><p>I completed my bachelor in Computer Science in 2012 at the Thomas More Colleage in Geel, Belgium</p></article></main><footer id=footer></footer></body></html>